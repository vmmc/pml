---
title: "Practical Machine Learning"
output: html_document
---

First, I enabled my cores to reduce the time of the computation.
```{r eval=FALSE}
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

Include caret library and set the seed for reproducible data.
```{r eval=FALSE}
library(caret)
set.seed(3439)
```

I load the data from the training set, remove columns that are mostly empty and some columns with low variance. 
```{r eval=FALSE}
data = read.csv("pml-training.csv", header = TRUE)


sub_data <- data[,colSums(is.na(data)) < (0.9 * nrow(data))]
sub_data <- sub_data[,colSums(sub_data=="") < (0.9 * nrow(sub_data))]
sub_data <- sub_data[sapply(sub_data, is.numeric)]

sub_data <- subset(sub_data, select = -c(X, raw_timestamp_part_1, raw_timestamp_part_2, num_window,
          gyros_dumbbell_x, gyros_dumbbell_y, gyros_dumbbell_z,
          gyros_forearm_x, gyros_forearm_y, gyros_forearm_z))


data <- data.frame(data$classe, sub_data)
```

Then divide the dataset into training(60%) and testing(40%) sets for cross-validation.
```{r eval=FALSE}
inTrain = createDataPartition(y=data$data.classe, p = 0.6, list=FALSE)
training = data[ inTrain,]
testing = data[-inTrain,]
```

Use the Random Forest (rf) algorithm which doesn't suffer from high number of features (around 50 in this case) since it only takes a random subset of them to build each tree. I avoid formulas on the train function to save time.
```{r eval=FALSE}
predictors = training$data.classe
decision = training[,-1]
modFit <- train(decision, predictors, method="rf")
```
It took around 53 minutes.

Now, having the model trained, run a prediction on the testing set to get out of sample error.
```{r eval=FALSE}
pred <- predict(modFit, testing)
confusionMatrix(testing$data.classe, predict(modFit, testing))
```
The results seem good:
```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2227    4    0    0    1
         B    9 1498   10    1    0
         C    0    1 1363    4    0
         D    0    1   21 1262    2
         E    0    1    4    6 1431

Overall Statistics
                                          
               Accuracy : 0.9917          
                 95% CI : (0.9895, 0.9936)
    No Information Rate : 0.285           
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9895          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9960   0.9953   0.9750   0.9914   0.9979
Specificity            0.9991   0.9968   0.9992   0.9963   0.9983
Pos Pred Value         0.9978   0.9868   0.9963   0.9813   0.9924
Neg Pred Value         0.9984   0.9989   0.9946   0.9983   0.9995
Prevalence             0.2850   0.1918   0.1782   0.1622   0.1828
Detection Rate         0.2838   0.1909   0.1737   0.1608   0.1824
Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838
Balanced Accuracy      0.9975   0.9961   0.9871   0.9939   0.9981
```

Finally, predict the 20 cases from the original test set.
```{r eval=FALSE}
test = read.csv("pml-testing.csv", header = TRUE)
test = test[, names(test) %in% colnames(data)]

g=predict(modFit, test)

print(g)
```
And the results were correct. 20/20
```
B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```
